{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function is getting the whole page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function is for getting detailed data of a product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detail_data(soup):\n",
    "    try:\n",
    "        name = soup.find('h1', id='itemTitle').text.strip().replace('Details about  \\xa0', ' ')\n",
    "    except:\n",
    "        name = ''\n",
    "    try:\n",
    "        price = soup.find('div', id='vi-mskumap-none').find('span').text.strip()\n",
    "    except:\n",
    "        price = ''\n",
    "    try:\n",
    "        sold = soup.find('span', class_='vi-qtyS-hot-red').find('a').text.strip().split(' ')[0]\n",
    "    except:\n",
    "        sold = ''\n",
    "    product_details = {\n",
    "        'name' : name,\n",
    "        'price' : price,\n",
    "        'total_sold' : sold\n",
    "    }\n",
    "    \n",
    "    return product_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function is getting all urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_links(soup):\n",
    "    try:\n",
    "        links = soup.find_all(\"a\", class_=\"s-item__link\")\n",
    "    except:\n",
    "        links = []\n",
    "        \n",
    "    urls = [item.get('href') for item in links]\n",
    "    \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    base_url = \"https://www.ebay.com/sch/i.html?_from=R40&_nkw=\"\n",
    "    url_separator = \"&_sacat=0&_pgn=\"\n",
    "    \n",
    "    print(\"Hello! this is the ebay data scraping robot.\")\n",
    "    print(\"---------------------------------------------\")\n",
    "    print(\"Enter name of the product you want to extract\")\n",
    "    request = input()\n",
    "    print(\"What kind of extration you want?\")\n",
    "    print(\"--------------------------------\")\n",
    "    print(\"1. Single Page\")\n",
    "    print(\"2. Multiple pages\")\n",
    "    type_of_scraping = input()\n",
    "    \n",
    "    #------Single page\n",
    "    if(type_of_scraping == \"Single\"):\n",
    "        print(\"Enter the page number you want to scrap\")\n",
    "        print(\"---------------------------------------\")\n",
    "        page_num = input()\n",
    "        url = base_url + request + url_separator + page_num\n",
    "        products = get_page_links(get_page(url))\n",
    "        for link in products:\n",
    "            data = detail_data(get_page(link))\n",
    "            print(data)\n",
    "        \n",
    "    #------Multiple pages\n",
    "    elif(type_of_scraping == \"Multiple\"):\n",
    "        print(\"First page number\")\n",
    "        start_page = input()\n",
    "        print(\"Last page number\")\n",
    "        end_page = input()\n",
    "        for pages in range(start_page, end_page):\n",
    "            url = base_url + request + url_separator + str(pages)\n",
    "            products = get_page_links(get_page(url))\n",
    "            for link in products:\n",
    "                data = detail_data(get_page(link))\n",
    "                print(data)\n",
    "                \n",
    "    else:\n",
    "        print(\"Sorry unable to detect:(\")\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
